{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tired-transaction",
   "metadata": {},
   "source": [
    "# [ E-6 ] 프로젝트: 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-williams",
   "metadata": {},
   "source": [
    "## 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-protection",
   "metadata": {},
   "source": [
    "wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
    "unzip song_lyrics.zip -d ~/aiffel/lyricist/data/lyrics  #lyrics 폴더에 압축풀기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-blues",
   "metadata": {},
   "source": [
    "## 2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-feature",
   "metadata": {},
   "source": [
    "**glob 를 활용하여 모든 txt 파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "anticipated-cloud",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['\"Don\\'t worry about a thing,', \"'Cause every little thing gonna be all right.\", 'Singin\\': \"Don\\'t worry about a thing,']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()                      # 텍스트를 라인 단위로 끊어서 list 형태로 읽어옵니다.\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-graphic",
   "metadata": {},
   "source": [
    "## 3. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-robertson",
   "metadata": {},
   "source": [
    "> - preprocess_sentence() 함수를 활용해 데이터를 정제한다.\n",
    "> - 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "approved-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf    # 텐서플로우\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "horizontal-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if len(sentence) > 15: continue    # 길이가 15 이상인 문장은 건너뜁니다.     \n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다. \n",
    "        \n",
    "    \n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 본다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-africa",
   "metadata": {},
   "source": [
    "**전처리를 위해 정규표현식 이용한 필터링을 사용한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cutting-august",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-immune",
   "metadata": {},
   "source": [
    "**정제함수를 활용해서 정제 데이터를 구축한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "subsequent-radius",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> don t worry about a thing , <end>',\n",
       " '<start> cause every little thing gonna be all right . <end>',\n",
       " '<start> singin don t worry about a thing , <end>',\n",
       " '<start> cause every little thing gonna be all right ! rise up this mornin , <end>',\n",
       " '<start> smiled with the risin sun , <end>',\n",
       " '<start> three little birds <end>',\n",
       " '<start> perch by my doorstep <end>',\n",
       " '<start> singin sweet songs <end>',\n",
       " '<start> of melodies pure and true , <end>',\n",
       " '<start> sayin , this is my message to you ou ou singin don t worry bout a thing , <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]               # 앞의 10문장 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-chicken",
   "metadata": {},
   "source": [
    "**정제된 데이터를 토큰화 한다(corpus를 텐서로 변환).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-finland",
   "metadata": {},
   "source": [
    "토큰화(Tokenize): 문장을 일정한 기준으로 자르는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "federal-albuquerque",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   37   15 ...    0    0    0]\n",
      " [   2   67  133 ...    0    0    0]\n",
      " [   2 1551   37 ...    0    0    0]\n",
      " ...\n",
      " [   5   22  712 ... 4178    4    3]\n",
      " [   5   22  712 ... 4178    4    3]\n",
      " [   5   22  712 ... 4178   20    3]] <keras_preprocessing.text.Tokenizer object at 0x7fc138b8a3c8>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen =15, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-cherry",
   "metadata": {},
   "source": [
    "> **check point)**  \"maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\"  어떻게 조정하는가?\n",
    "> - \"tf.keras.preprocessing.sequence.pad_sequences\"에 maxlen 설정하기\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-terminal",
   "metadata": {},
   "source": [
    "**소스문장과 타겟문장을 생성한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-logging",
   "metadata": {},
   "source": [
    "\"start\" 가 문장의 시작에 더해진 입력 데이터(문제지)와, \"end\" 가 문장의 끝에 더해진 출력 데이터(답안지)가 필요하며, 이는 문장 데이터만 있으면 만들어낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "established-payroll",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  37  15 717 113   9 181   4   3   0   0   0   0   0]\n",
      "[ 37  15 717 113   9 181   4   3   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])             # 소스 문장 확인\n",
    "print(tgt_input[0])             # 타겟 문장 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-brown",
   "metadata": {},
   "source": [
    "0은 정해진 입력 시퀀스 길이보다 문장이 짧을 경우, 0으로 패딩을 채워넣은 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-barrier",
   "metadata": {},
   "source": [
    "**데이터셋 객체를 생성한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "separated-rabbit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 개수와, 여기 포함되지 않은 0:<pad>를 포함\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)  # corpus 텐서를 dataset 객체로 변환\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-david",
   "metadata": {},
   "source": [
    "Batch size는 256, LSTM에 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스 출력값 14의 Dataset이 생성되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-sympathy",
   "metadata": {},
   "source": [
    "## 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-quantity",
   "metadata": {},
   "source": [
    "> - tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리한다.\n",
    "> - 단어장의 크기는 12,000 이상으로 설정한다.\n",
    "> - 총 데이터의 20%를 평가 데이터셋으로 사용한다.\n",
    "> - 만약 학습데이터 갯수가 124960보다 크다면 위 Step 3.의 데이터 정제 과정을 다시한번 검토해 본다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-dividend",
   "metadata": {},
   "source": [
    "**훈련데이터와 평가데이터를 분리한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "requested-champagne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140599, 14)\n",
      "Target Train: (140599, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-cylinder",
   "metadata": {},
   "source": [
    "**옵션 값**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-islam",
   "metadata": {},
   "source": [
    "- test_size:  테스트 셋 구성의 비율을 나타낸다. train_size의 옵션과 반대 관계에 있는 옵션 값이며, 주로 test_size를 지정해 준다. 0.2는 전체 데이터 셋의 20%를 test (validation) 셋으로 지정하겠다는 의미이며, 참고로 default 값은 0.25이다.\n",
    "- shuffle:  split을 해주기 이전에 섞을건지 여부이다. default=True이며, 보통은 default 값으로 둔다.\n",
    "- stratify:  classification을 다룰 때 매우 중요한 옵션값이다. stratify 값을 target으로 지정해주면 각각의 class 비율(ratio)을 train / validation에 유지해 준다.(즉, 한 쪽에 쏠려서 분배되는 것을 방지한다) 만약 이 옵션을 지정해 주지 않고 classification 문제를 다룬다면, 성능의 차이가 많이 날 수 있습니다. default=None 이다.\n",
    "- random_state:  세트를 섞을 때 해당 int 값을 보고 섞으며, 하이퍼 파라미터를 튜닝시 이 값을 고정해두고 튜닝해야 매번 데이터셋이 변경되는 것을 방지할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-number",
   "metadata": {},
   "source": [
    "**추가) numpy random에서도 데이터 분리 가능**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-chassis",
   "metadata": {},
   "source": [
    "1. scikit-learn 라이브러리 model_selection 클래스의 train_test_split 함수를 이용하여 train, test set 분할하기\n",
    "2. numpy random 클래스의 permutation() 함수를 이용하여 train, test set 분할하기\n",
    "3. numpy random 클래스의 choice() 함수를 이용하여 train, test set 분할하기\n",
    "4. numpy random 클래스의 shuffle() 함수를 이용하여 train, test set 분할하기\n",
    "\n",
    "\n",
    "출처: https://rfriend.tistory.com/519 [R, Python 분석과 프로그래밍의 친구 (by R Friend)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-telling",
   "metadata": {},
   "source": [
    "## 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-momentum",
   "metadata": {},
   "source": [
    "> - 모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-fiction",
   "metadata": {},
   "source": [
    "**모델 설계하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "functional-tokyo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료\n"
     ]
    }
   ],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1000\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "print(\"완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-chile",
   "metadata": {},
   "source": [
    "> **check poin)** 적당한 embedding size와 hidden_size는 어떻게 정하는가?\n",
    "> - embedding size는 워드 벡터의 차원수, 즉 단어가 추상적으로 표현되는 크기이다.\n",
    "> - hidden size는 LSTM 레이어의 hidden state의 차원수 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "coordinate-feeling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=385, shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 1.83470111e-04, -2.33849103e-04,  1.33321402e-04, ...,\n",
       "         -4.80654235e-05, -1.51777795e-05,  1.70678337e-04],\n",
       "        [ 3.48550559e-04, -2.96735932e-04,  4.01951082e-04, ...,\n",
       "         -1.45048252e-04, -2.45581614e-04,  1.97778063e-04],\n",
       "        [ 3.70774651e-04, -3.91234120e-04,  6.19361235e-04, ...,\n",
       "         -9.63204584e-05, -3.51205323e-04, -7.79941547e-05],\n",
       "        ...,\n",
       "        [ 6.49349357e-04, -4.47583298e-04,  3.27827467e-04, ...,\n",
       "         -3.61214625e-04, -6.17709593e-04, -9.49099427e-04],\n",
       "        [ 6.69393106e-04, -4.14761540e-04,  2.58772314e-04, ...,\n",
       "         -2.94661673e-04, -3.94051633e-04, -9.70066874e-04],\n",
       "        [ 7.26587954e-04, -3.85876017e-04,  1.86687437e-04, ...,\n",
       "         -1.42125864e-04, -1.16386487e-04, -9.72599955e-04]],\n",
       "\n",
       "       [[ 1.83470111e-04, -2.33849103e-04,  1.33321402e-04, ...,\n",
       "         -4.80654235e-05, -1.51777795e-05,  1.70678337e-04],\n",
       "        [ 2.75023311e-04, -1.97330388e-04,  2.83266010e-04, ...,\n",
       "          3.89335801e-05,  9.37516452e-05, -1.21092526e-04],\n",
       "        [ 2.61513080e-04, -2.02148716e-04,  4.06929583e-04, ...,\n",
       "         -4.84505945e-05,  1.78805130e-04, -5.19683643e-04],\n",
       "        ...,\n",
       "        [-6.98620745e-04,  8.48431315e-04, -6.02297659e-04, ...,\n",
       "          1.01237942e-03,  2.07781047e-03, -1.41847238e-03],\n",
       "        [-8.07309523e-04,  9.51741065e-04, -9.14869539e-04, ...,\n",
       "          1.03566411e-03,  2.37259176e-03, -1.60164502e-03],\n",
       "        [-9.30148934e-04,  1.04599458e-03, -1.22717815e-03, ...,\n",
       "          1.02137879e-03,  2.65211728e-03, -1.76762836e-03]],\n",
       "\n",
       "       [[ 1.10446585e-04, -8.05452728e-05, -8.49044736e-05, ...,\n",
       "         -1.54847701e-04,  9.59918252e-05,  7.01682293e-05],\n",
       "        [-1.10160508e-04, -2.31730173e-05, -3.10318137e-04, ...,\n",
       "         -2.33808445e-04,  2.54033104e-04,  6.41121587e-05],\n",
       "        [-5.79033687e-04, -1.17333628e-04, -4.76081652e-04, ...,\n",
       "         -1.03731662e-04,  3.44815868e-04,  4.48091392e-04],\n",
       "        ...,\n",
       "        [-6.95959840e-04,  6.68514404e-04,  2.96265585e-04, ...,\n",
       "         -2.03167772e-04,  4.90878709e-04, -1.11925916e-03],\n",
       "        [-4.44025034e-04,  7.30164058e-04,  6.96013158e-04, ...,\n",
       "         -1.43814774e-04,  2.93318444e-04, -1.32923923e-03],\n",
       "        [-1.91270301e-04,  6.40006270e-04,  8.56133061e-04, ...,\n",
       "          1.21861565e-04,  9.58762175e-05, -1.11212046e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.83470111e-04, -2.33849103e-04,  1.33321402e-04, ...,\n",
       "         -4.80654235e-05, -1.51777795e-05,  1.70678337e-04],\n",
       "        [ 2.46771844e-04, -8.54286700e-05,  1.11193687e-04, ...,\n",
       "          1.13903377e-04,  6.80682715e-05,  2.04112104e-04],\n",
       "        [-1.23787584e-04, -1.77641035e-04,  1.13821559e-04, ...,\n",
       "          3.78680008e-04,  9.75341172e-05,  6.12207688e-04],\n",
       "        ...,\n",
       "        [-9.28729365e-04, -3.42234067e-04,  1.04525732e-03, ...,\n",
       "         -2.02362353e-04,  9.79352044e-04,  1.30328536e-03],\n",
       "        [-9.11652460e-04, -4.47004219e-04,  1.03798497e-03, ...,\n",
       "         -3.74681200e-04,  8.13273247e-04,  1.22851611e-03],\n",
       "        [-8.52896424e-04, -5.60107990e-04,  9.01671825e-04, ...,\n",
       "         -3.51517345e-04,  8.13464285e-04,  1.01525395e-03]],\n",
       "\n",
       "       [[ 1.83470111e-04, -2.33849103e-04,  1.33321402e-04, ...,\n",
       "         -4.80654235e-05, -1.51777795e-05,  1.70678337e-04],\n",
       "        [ 3.48550559e-04, -2.96735932e-04,  4.01951082e-04, ...,\n",
       "         -1.45048252e-04, -2.45581614e-04,  1.97778063e-04],\n",
       "        [ 4.99504036e-04, -3.25181376e-04,  2.04953380e-04, ...,\n",
       "         -1.88128488e-05, -3.95753828e-04,  9.12106771e-05],\n",
       "        ...,\n",
       "        [ 4.02556907e-04, -6.78337179e-04,  5.95049176e-04, ...,\n",
       "          1.19897188e-03,  5.91763761e-04, -1.17556879e-03],\n",
       "        [ 2.34150066e-04, -3.45143577e-04,  3.47381399e-04, ...,\n",
       "          1.21558155e-03,  9.90815228e-04, -1.36488804e-03],\n",
       "        [ 4.01727666e-05, -2.99247840e-05,  4.04718048e-05, ...,\n",
       "          1.19989295e-03,  1.39026553e-03, -1.54347194e-03]],\n",
       "\n",
       "       [[ 1.83470111e-04, -2.33849103e-04,  1.33321402e-04, ...,\n",
       "         -4.80654235e-05, -1.51777795e-05,  1.70678337e-04],\n",
       "        [ 3.48550559e-04, -2.96735932e-04,  4.01951082e-04, ...,\n",
       "         -1.45048252e-04, -2.45581614e-04,  1.97778063e-04],\n",
       "        [ 1.68051032e-04, -3.00648622e-04,  7.58200069e-04, ...,\n",
       "         -4.39614414e-05, -4.14876733e-04,  1.87513535e-04],\n",
       "        ...,\n",
       "        [ 7.49882893e-04, -4.27131046e-04,  1.07838283e-03, ...,\n",
       "         -4.34718124e-04, -6.05417939e-04,  1.19653880e-03],\n",
       "        [ 7.36111891e-04, -3.89672932e-04,  9.58389661e-04, ...,\n",
       "         -2.73758167e-04, -2.99273815e-04,  1.28881331e-03],\n",
       "        [ 7.27136910e-04, -2.79987085e-04,  9.14948178e-04, ...,\n",
       "         -4.96340799e-05,  3.39978724e-05,  1.22713461e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-directory",
   "metadata": {},
   "source": [
    "**결과)** shape=(256, 14, 12001)에서 256은 배치 사이즈, 14는 입력-출력 시퀀스 길이, 12001은 Dense 레이어의 출력 차원수이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "resident-illinois",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                multiple                  5028000   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                multiple                  8004000   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  12013001  \n",
      "=================================================================\n",
      "Total params: 28,117,257\n",
      "Trainable params: 28,117,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-resort",
   "metadata": {},
   "source": [
    "**인공지능 학습시키기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "realistic-pavilion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "686/686 [==============================] - 89s 130ms/step - loss: 3.6345\n",
      "Epoch 2/10\n",
      "686/686 [==============================] - 90s 132ms/step - loss: 3.1671\n",
      "Epoch 3/10\n",
      "686/686 [==============================] - 99s 144ms/step - loss: 2.9763\n",
      "Epoch 4/10\n",
      "686/686 [==============================] - 98s 142ms/step - loss: 2.8269\n",
      "Epoch 5/10\n",
      "686/686 [==============================] - 98s 143ms/step - loss: 2.6977\n",
      "Epoch 6/10\n",
      "686/686 [==============================] - 98s 143ms/step - loss: 2.5822\n",
      "Epoch 7/10\n",
      "686/686 [==============================] - 98s 143ms/step - loss: 2.4745\n",
      "Epoch 8/10\n",
      "686/686 [==============================] - 98s 143ms/step - loss: 2.3729\n",
      "Epoch 9/10\n",
      "686/686 [==============================] - 98s 143ms/step - loss: 2.2780\n",
      "Epoch 10/10\n",
      "686/686 [==============================] - 98s 143ms/step - loss: 2.1873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc05857f668>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)                   # 10으로 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-auction",
   "metadata": {},
   "source": [
    "**결과)** 10 Epoch 안에 loss가 2.1873으로 2.2 수준으로 낮추어졌다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-scottish",
   "metadata": {},
   "source": [
    "**모델 평가하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "subjective-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    \n",
    "    \n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "preceding-wayne",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i love you <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-villa",
   "metadata": {},
   "source": [
    "## 모델이 생성한 문장 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-ownership",
   "metadata": {},
   "source": [
    "> 'i love you , i love you'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-situation",
   "metadata": {},
   "source": [
    "### TEST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "stretch-monkey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> love is a beautiful thing <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cultural-orchestra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you know i m bad , i m bad you know it <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cellular-payday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you know i m bad , i m bad you know it <end> '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "activated-stage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> bad boy , fatty girl <end> '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> bad\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adjacent-peripheral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> winter s gone , i m the only one <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> winter\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "younger-current",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> dream , i m ready <end> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> dream\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "rapid-advantage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> cool , easy , easy <end> '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> cool\", max_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "emotional-clinic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> snow in the sky <end> '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> snow\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "decimal-modification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> movie <end> '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> movie\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "controversial-produce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> how many times i gotta do ? <end> '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> how\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "later-observer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> how are you ? <end> '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> how are\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "favorite-internet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> what you want nixga what you what you want nixga <end> '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> what\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-nomination",
   "metadata": {},
   "source": [
    "## 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-davis",
   "metadata": {},
   "source": [
    "> 최종적으로 Embedding Size = 256 Hidden Size = 1000 으로 설정하고, 10 Epoch 안에 loss가 2.1873으로 2.2 수준으로 낮추어졌다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-guard",
   "metadata": {},
   "source": [
    "## 총평"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-albuquerque",
   "metadata": {},
   "source": [
    "> input을 다양하게 해서 테스트를 했더니, 생각보다 완결된 문장을 출력했다. 그러나 같은 단어에 대해서는 같은 문장을 출력했고, 'movie'와 같은 단어의 경우엔 추가적으로 문장이 생성되지 않았다. 'movie' 라는 단어가 dataset에 없어서 그런 것일까 생각했다. \n",
    ">\n",
    ">그리고 'what' 을 넣으니,  'what you want nixga what you what you want nixga'가 출력되어 흠칫 놀랬다. (racism적 단어인 줄 알고) 더불어 얼마전 논란이 된 AI 챗봇 '이루다'가 떠올랐다. '이루다' 또한 기본적으로 이러한 매커니즘을 통해서 작동을 했을 것이다. 사용자가 던지는 질문에 대답 하기 위해서는 다양하고 규모가 큰 data가 전제되어야 한다. 더욱이 사람과 대화하는 듯한 느낌을 주기 위해서는 실제 사람들의 대화 내용을 기반으로 하는 것이 논리적인 선택이었을 것이다. 그러나 이번 논란에서도 그 이전의 논란에서도 많은 데이터가 필요한 만큼 어떤 데이터를 사용하는가도 동시에 중요한 문제가 된다. AI는 도덕적 판단을 하는 것이 아니라 데이터를 기반으로 하나의 확률을 출력한 것이기 때문이다. 데이터의 정제는 여전히 사람의 영역인 듯 하다. \n",
    ">\n",
    ">good data가 good output으로 이어진다.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
